{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network For Biggner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Simple neural network.</h1>\n",
    "<h3>Muhammad Abbas davi</h3>\n",
    "<p>in this code i will show how to design the simple neural network and how its works.</p>\n",
    "<h2>I have split this code into five parts.</h2>\n",
    "<ol>\n",
    "    <li>Importing all pakages which is required</li>\n",
    "    <li>Then we will provide Inputs and Output</li>\n",
    "    <li>Design Our Model</li>\n",
    "    <li>Selecting Optimization and Loss Function.</li>\n",
    "    <li>And Finally we Train Our Model</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing All Pakages which is Required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tensorflow</h2>\n",
    "<p>TensorFlow is a free and open-source software library for <b><i>Machine Learning (ML)</i></b>. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks.\n",
    "\n",
    "Tensorflow is a symbolic math library based on dataflow and differentiable programming. It is used for both research and production at Google.\n",
    "\n",
    "TensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache License 2.0 in 2015.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NumPy</h2>\n",
    "<p>NumPy (pronounced numpi (NUM-py) or sometimes numpee NUM-pee) is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.The ancestor of NumPy, Numeric, was originally created by Jim Hugunin with contributions from several other developers. In 2005, Travis Oliphant created NumPy by incorporating features of the competing Numarray into Numeric, with extensive modifications. NumPy is open-source software and has many contributors.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#For Ploting we Just Import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Provide Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "#now Create the simple Input which name is x\n",
    "x = np.array([\n",
    "    [1,2,3,4,5,6,7,8,9]\n",
    "]).T\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10]\n",
      " [20]\n",
      " [30]\n",
      " [40]\n",
      " [50]\n",
      " [60]\n",
      " [70]\n",
      " [80]\n",
      " [90]]\n"
     ]
    }
   ],
   "source": [
    "# Now Create the Simple Output which name is y.\n",
    "y = np.array([\n",
    "    [10,20,30,40,50,60,70,80,90]\n",
    "]).T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the x shape is (9, 1) where the 9 is examples and the 1 is feature.\n",
      "the y shape is (9, 1) where the 9 is examples and the 1 is feature.\n"
     ]
    }
   ],
   "source": [
    "#Now We Check the Shape Of Our Input and Output\n",
    "print(\"the x shape is {} where the {} is examples and the {} is feature.\".format(x.shape,x.shape[0],x.shape[1]))\n",
    "print(\"the y shape is {} where the {} is examples and the {} is feature.\".format(y.shape,y.shape[0],y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 1)\n",
      "(9, 1)\n"
     ]
    }
   ],
   "source": [
    "#you can also print shape like that.\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>here is only one feature and this feature have 9 examples.</h3>\n",
    "<p>basically the feature mean Input. and example is sub-part of feature mean x is the feature and the 9 values are Examples<br>\n",
    "if we talk about the feature so its mean all value that include in input. if we talk about the example so thats mean it is the one of feature.<br>\n",
    "feature may be one or more then one it is not fixed.<br>\n",
    "and each feature have one or many examples.<br>\n",
    "</p>\n",
    "<img src='image1.png'>\n",
    "in the above table the overall x is the feature and the only 1 is the example of this feature.<br>\n",
    "<h3>Here are terminology Of Machine Learning</h3>\n",
    "<img src='image2.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Design Our Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now We design the Model.\n",
    "model = tf.keras.Sequential(name='Simple_Neural_Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Add the Layers.\n",
    "model.add(tf.keras.layers.Dense(units = 1 , input_shape = [1] , name = 'Input_Layer_with_one_Neuron'))\n",
    "model.add(tf.keras.layers.Dense(units = 2 , name = 'Hidden_Layer_1_with_Two_Neuron'))\n",
    "model.add(tf.keras.layers.Dense(units = 1 , name = 'Output_Layer_with_1_Neuron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simple_Neural_Network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_Layer_with_one_Neuron  (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "Hidden_Layer_1_with_Two_Neur (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "Output_Layer_with_1_Neuron ( (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the Units mean how many neurons we wants.\n",
    "#Now Print Model is created and Now we Check the Summary of Our Model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>in the above summary there are first input layer have 2 perameters in which 1 is input and another is the bias value. similarly the second layer have 2 bias and the first neuron output is the input for these two neurons show below thats why its have 4 perameters.\n",
    "and the last layer have 3 perameters in which the two is the output of two neurons and one is the bias.\n",
    "    \n",
    "</h3>\n",
    "\n",
    "<h2>Our Model</h2>\n",
    "\n",
    "<img src='Output.png'>\n",
    "\n",
    "<h3>How Activation Function Look like.</h3>\n",
    "<img src = 'activ_func.png'>\n",
    "<h3>what is the activation Function</h3>\n",
    "<p>In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities.</p>\n",
    "\n",
    "<img src='activation.png'>\n",
    "\n",
    "<h3>what is the bias</h3>\n",
    "<p>The activation function in Neural Networks takes an input 'x' multiplied by a weight 'w'. Bias allows you to shift the activation function by adding a constant (i.e. the given bias) to the input. Bias in Neural Networks can be thought of as analogous to the role of a constant in a linear function, whereby the line is effectively transposed by the constant value.\n",
    "<img src='no bias.png'>\n",
    "In a scenario with no bias, the input to the activation function is 'x' multiplied by the connection weight 'w0'.\n",
    "<img src='with bias.png'>\n",
    "In a scenario with bias, the input to the activation function is 'x' times the connection weight 'w0' plus the bias times the connection weight for the bias 'w1'. This has the effect of shifting the activation function by a constant amount (b * w1).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimization and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In deep learning, we have the concept of loss, which tells us how poorly the model is performing at that current instant. Now we need to use this loss to train our network such that it performs better. Essentially what we need to do is to take the loss and try to minimize it, because a lower loss means our model is going to perform better. The process of minimizing (or maximizing) any mathematical expression is called optimization.<br>\n",
    "\n",
    "Optimizers are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizers are used to solve optimization problems by minimizing the function.</p>\n",
    "\n",
    "<h3>How do Optimizers work?</h3>\n",
    "<p>For a useful mental model, you can think of a hiker trying to get down a mountain with a blindfold on. It’s impossible to know which direction to go in, but there’s one thing she can know: if she’s going down (making progress) or going up (losing progress). Eventually, if she keeps taking steps that lead her downwards, she’ll reach the base.<br>\n",
    "\n",
    "Similarly, it’s impossible to know what your model’s weights should be right from the start. But with some trial and error based on the loss function (whether the hiker is descending), you can end up getting there eventually.<br>\n",
    "\n",
    "How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use. Optimization algorithms are responsible for reducing the losses and to provide the most accurate results possible.<br>\n",
    "\n",
    "Various optimizers are researched within the last few couples of years each having its advantages and disadvantages. Read the entire article to understand the working, advantages, and disadvantages of the algorithms.</p>\n",
    "\n",
    "<h3>There are Different Type of Optimizers.</h3>\n",
    "<ol>\n",
    "    <li>Gradient Descent</li>\n",
    "    <li>Stochastic Gradient Descent (SGD)</li>\n",
    "    <li>Mini Batch Stochastic Gradient Descent (MB-SGD)</li>\n",
    "    <li>SGD with momentum</li>\n",
    "    <li>Nesterov Accelerated Gradient (NAG)</li>\n",
    "    <li>Adaptive Gradient (AdaGrad)</li>\n",
    "    <li>AdaDelta</li>\n",
    "    <li>RMSprop</li>\n",
    "    <li>Adam</li>\n",
    "</ol>\n",
    "\n",
    "<h3>In this Code we will use the Adam.</h3>\n",
    "<h3>Adam</h3>\n",
    "<p>Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum.<br>\n",
    "\n",
    "Adam computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.</p>\n",
    "\n",
    "<h3>Mean Squared Error</h3>\n",
    "<p>The Mean Squared Error, or MSE, loss is the default loss to use for regression problems.<br>\n",
    "\n",
    "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian. It is the loss function to be evaluated first and only changed if you have a good reason.<br>\n",
    "\n",
    "Mean squared error is calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0. The squaring means that larger mistakes result in more error than smaller mistakes, meaning that the model is punished for making larger mistakes.<br>\n",
    "\n",
    "The mean squared error loss function can be used in Keras by specifying ‘mse‘ or ‘mean_squared_error‘ as the loss function when compiling the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we Compile the model mean select the Optimize and loss function.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.5),\n",
    "             loss=tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train Our Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0164\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0659\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0800\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0332\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0453\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0552\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0236\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0094\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0297\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0356\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0216\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0066\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0023\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0126\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0026\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0083\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0075\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.3688e-04\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0016\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0062\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0045\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2661e-04\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0021\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0047\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0027\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4511e-04\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0022\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0015\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.8821e-0 - 0s 2ms/step - loss: 5.8821e-04\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0020\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0022\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 6.6718e-04\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7414e-04\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0604e-04\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3382e-04\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.5400e-04\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.1161e-05\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8223e-04\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.8285e-04\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0912e-04\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.0932e-05\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1060e-04\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.8508e-04\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0464e-05\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6058e-04\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0890e-04\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1494e-04\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4415e-05\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9890e-04\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.9147e-04\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.1915e-05\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1239e-05\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7149e-04\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.9386e-05\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7285e-0 - 0s 2ms/step - loss: 2.7285e-05\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.8842e-05\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0518e-04\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.7368e-05\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5361e-05\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0066e-05\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 4.1621e-05\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.3436e-06\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0641e-05\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4483e-05\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6873e-06\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1602e-05\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.3672e-05\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4748e-05\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3407e-07\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8545e-05\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9258e-05\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8543e-06\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3123e-06\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7297e-05\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8165e-06\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3760e-06\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1823e-05\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.7448e-06\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9226e-06\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3021e-06\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.2999e-06\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.0567e-06\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5915e-06\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.7161e-06\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8383e-06\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.8329e-07\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7891e-06\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6785e-06\n",
      "Epoch 100/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2976e-07\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5495e-06\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8860e-06\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5834e-07\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6854e-07\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9196e-06\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0393e-06\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7050e-08\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1151e-06\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1346e-06\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 9.8866e-08\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7220e-07\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0509e-06\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8910e-07\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8755e-07\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 8.1887e-07\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0902e-07\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4737e-07\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 5.4769e-07\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2748e-07\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.1343e-08\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2261e-07\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7692e-07\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2853e-08\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.5654e-07\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7397e-07\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1801e-08\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3141e-08\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9092e-07\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8288e-08\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1284e-08\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2699e-07\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1787e-08\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5963e-09\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.5375e-08\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8053e-08\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7565e-09\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2007e-08\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.1047e-08\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3004e-08\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1409e-08\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4433e-08\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6436e-08\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.7423e-08\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9417e-08\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6443e-08\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0008e-09\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8015e-08\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2824e-08\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7238e-09\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.7914e-08\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6876e-09\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3246e-09\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1055e-08\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9433e-09\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1935e-10\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4329e-09\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9053e-09\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2798e-10\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8508e-09\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.1191e-09\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1983e-10\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7735e-09\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0398e-09\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.7327e-10\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7346e-09\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7564e-09\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4598e-10\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2701e-09\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 3.9574e-09\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.2355e-10\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6593e-09\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7650e-09\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9533e-10\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1710e-09\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 2.1864e-09\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2379e-10\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.8257e-10\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0599e-09\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0358e-10\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3923e-10\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0433e-09\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.2439e-11\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1837e-10\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6809e-10\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9420e-11\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3526e-10\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4334e-10\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.9631e-11\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2693e-10\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 1.2693e-10\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.7922e-11\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 999us/step - loss: 5.9420e-11\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.5273e-11\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9315e-11\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8414e-11\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9310e-11\n",
      "Epoch 197/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4460e-11\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.6292e-11\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.1230e-11\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.6977e-11\n"
     ]
    }
   ],
   "source": [
    "# Now after the 4 steps we train out model.\n",
    "data = model.fit(x=x , y=y , epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now Check the Loss Of out model.\n",
    "plt.plot(data.history['loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200.00002]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we chekc the prediction of 20 , 30 , 46\n",
    "model.predict([20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[300.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[320.00003]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hence the prediction is accurate with less error. So we says that our Neural Network Learn very Well.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
